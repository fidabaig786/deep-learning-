{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "xSjB7HQhJg0j"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "1an6FB3WKgXg"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(x, y, weights, bias): \n",
    "    y_pred = predict(x, weights, bias)\n",
    "    loss = (y_pred - y)**2   \n",
    "    d_loss = 2*(y_pred - y)\n",
    "    \n",
    "    return y_pred, loss, d_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Bc_65bbDK_T6"
   },
   "outputs": [],
   "source": [
    "def optimize_perceptron(x, y, learning_rate):\n",
    "    epoch = 0\n",
    "    error = 999\n",
    "    weights = np.random.rand(x.shape[1])\n",
    "    bias = np.random.rand()\n",
    "    \n",
    "    errors = list()\n",
    "    epochs = list()\n",
    "    \n",
    "    # Loop until stop conditions are met\n",
    "    while (epoch <= 20) and (error > 9e-4):\n",
    "        \n",
    "        loss_ = 0\n",
    "        # Loop over every data point\n",
    "        for i in range(x.shape[0]):\n",
    "            \n",
    "            # Forward Propagation on each data point\n",
    "            y_pred, loss, d_loss = forward_propagation(x[i], y[i], weights, bias)\n",
    "\n",
    "            # Backpropagation\n",
    "            partial_derivates = backpropagation(x[i], d_loss)\n",
    "            \n",
    "            # Learn by updating the weights of the perceptron\n",
    "            weights = weights - (learning_rate * np.array(partial_derivates))\n",
    "\n",
    "        # Evaluate the results\n",
    "        for index, feature_value_test in enumerate(x):\n",
    "            y_pred, loss, d_loss = forward_propagation(feature_value_test, y[index], weights, bias)\n",
    "            loss_ += loss\n",
    "\n",
    "        errors.append(loss_/len(x))\n",
    "        epochs.append(epoch)\n",
    "        error = errors[-1]\n",
    "        epoch += 1\n",
    "\n",
    "        print('Epoch {}. loss: {}'.format(epoch, errors[-1]))\n",
    "\n",
    "    \n",
    "    return weights, bias, errors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "UGdGfKN2rmqE"
   },
   "outputs": [],
   "source": [
    "def activation_function(prediction):\n",
    "    if prediction.any() >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def predict(x, weights, bias):\n",
    "\n",
    "    prediction = np.dot(weights, x) + bias\n",
    "    prediction = activation_function(prediction)\n",
    "    \n",
    "    return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "jRvrWByLsT4u"
   },
   "outputs": [],
   "source": [
    "def backpropagation(x, d_loss):\n",
    "    partial_derivates = list()\n",
    "    for feature_value in x:\n",
    "        partial_derivates.append(d_loss*feature_value)\n",
    "        \n",
    "    return partial_derivates   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76VkytRer73n",
    "outputId": "50b376e4-95af-4014-f52f-172641fda41e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n",
      "Values before rescaling:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255]\n",
      "Values after rescaling:  [0.         0.00392157 0.00784314 0.01176471 0.01568627 0.01960784\n",
      " 0.02352941 0.02745098 0.03137255 0.03529412 0.03921569 0.04313725\n",
      " 0.04705882 0.05098039 0.05490196 0.05882353 0.0627451  0.06666667\n",
      " 0.07058824 0.0745098  0.07843137 0.08235294 0.08627451 0.09019608\n",
      " 0.09411765 0.09803922 0.10196078 0.10588235 0.10980392 0.11372549\n",
      " 0.11764706 0.12156863 0.1254902  0.12941176 0.13333333 0.1372549\n",
      " 0.14117647 0.14509804 0.14901961 0.15294118 0.15686275 0.16078431\n",
      " 0.16470588 0.16862745 0.17254902 0.17647059 0.18039216 0.18431373\n",
      " 0.18823529 0.19215686 0.19607843 0.2        0.20392157 0.20784314\n",
      " 0.21176471 0.21568627 0.21960784 0.22352941 0.22745098 0.23137255\n",
      " 0.23529412 0.23921569 0.24313725 0.24705882 0.25098039 0.25490196\n",
      " 0.25882353 0.2627451  0.26666667 0.27058824 0.2745098  0.27843137\n",
      " 0.28235294 0.28627451 0.29019608 0.29411765 0.29803922 0.30196078\n",
      " 0.30588235 0.30980392 0.31372549 0.31764706 0.32156863 0.3254902\n",
      " 0.32941176 0.33333333 0.3372549  0.34117647 0.34509804 0.34901961\n",
      " 0.35294118 0.35686275 0.36078431 0.36470588 0.36862745 0.37254902\n",
      " 0.37647059 0.38039216 0.38431373 0.38823529 0.39215686 0.39607843\n",
      " 0.4        0.40392157 0.40784314 0.41176471 0.41568627 0.41960784\n",
      " 0.42352941 0.42745098 0.43137255 0.43529412 0.43921569 0.44313725\n",
      " 0.44705882 0.45098039 0.45490196 0.45882353 0.4627451  0.46666667\n",
      " 0.47058824 0.4745098  0.47843137 0.48235294 0.48627451 0.49019608\n",
      " 0.49411765 0.49803922 0.50196078 0.50588235 0.50980392 0.51372549\n",
      " 0.51764706 0.52156863 0.5254902  0.52941176 0.53333333 0.5372549\n",
      " 0.54117647 0.54509804 0.54901961 0.55294118 0.55686275 0.56078431\n",
      " 0.56470588 0.56862745 0.57254902 0.57647059 0.58039216 0.58431373\n",
      " 0.58823529 0.59215686 0.59607843 0.6        0.60392157 0.60784314\n",
      " 0.61176471 0.61568627 0.61960784 0.62352941 0.62745098 0.63137255\n",
      " 0.63529412 0.63921569 0.64313725 0.64705882 0.65098039 0.65490196\n",
      " 0.65882353 0.6627451  0.66666667 0.67058824 0.6745098  0.67843137\n",
      " 0.68235294 0.68627451 0.69019608 0.69411765 0.69803922 0.70196078\n",
      " 0.70588235 0.70980392 0.71372549 0.71764706 0.72156863 0.7254902\n",
      " 0.72941176 0.73333333 0.7372549  0.74117647 0.74509804 0.74901961\n",
      " 0.75294118 0.75686275 0.76078431 0.76470588 0.76862745 0.77254902\n",
      " 0.77647059 0.78039216 0.78431373 0.78823529 0.79215686 0.79607843\n",
      " 0.8        0.80392157 0.80784314 0.81176471 0.81568627 0.81960784\n",
      " 0.82352941 0.82745098 0.83137255 0.83529412 0.83921569 0.84313725\n",
      " 0.84705882 0.85098039 0.85490196 0.85882353 0.8627451  0.86666667\n",
      " 0.87058824 0.8745098  0.87843137 0.88235294 0.88627451 0.89019608\n",
      " 0.89411765 0.89803922 0.90196078 0.90588235 0.90980392 0.91372549\n",
      " 0.91764706 0.92156863 0.9254902  0.92941176 0.93333333 0.9372549\n",
      " 0.94117647 0.94509804 0.94901961 0.95294118 0.95686275 0.96078431\n",
      " 0.96470588 0.96862745 0.97254902 0.97647059 0.98039216 0.98431373\n",
      " 0.98823529 0.99215686 0.99607843 1.        ]\n"
     ]
    }
   ],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "\n",
    "# Split dataset with 75% training data and 25% test data\n",
    "\n",
    "\n",
    "# Split datasets into features and labels\n",
    "\n",
    "\n",
    "print(train_X.shape)\n",
    "print(test_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)\n",
    "print('Values before rescaling: ', np.unique(train_X))\n",
    "\n",
    "# Rescale data points to values between 0 and 1 (pixels are originally 0-255)\n",
    "train_X = train_X/ 255.\n",
    "test_X= test_X / 255.\n",
    "print('Values after rescaling: ', np.unique(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "_DGU5IEluYPy"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(x_test, y_test, weights, bias):\n",
    "    \n",
    "    # Initialize True Positive, True Negative, False Positive and False Negative\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "\n",
    "    for sample, label in zip(x_test, y_test):\n",
    "\n",
    "        prediction = predict(sample, weights, bias)\n",
    "\n",
    "        if prediction == label:\n",
    "            if prediction == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            if prediction == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "\n",
    "    accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kH2QZ2iIe0Bw",
    "outputId": "3241baf3-5b89-440b-8af0-13adc90e7a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1. loss: 20.2774\n",
      "Epoch 2. loss: 20.2774\n",
      "Epoch 3. loss: 20.2774\n",
      "Epoch 4. loss: 20.2774\n",
      "Epoch 5. loss: 20.2774\n",
      "Epoch 6. loss: 20.2774\n",
      "Epoch 7. loss: 20.2774\n",
      "Epoch 8. loss: 20.2774\n",
      "Epoch 9. loss: 20.2774\n",
      "Epoch 10. loss: 20.2774\n",
      "Epoch 11. loss: 20.2774\n",
      "Epoch 12. loss: 20.2774\n",
      "Epoch 13. loss: 20.2774\n",
      "Epoch 14. loss: 20.2774\n",
      "Epoch 15. loss: 20.2774\n",
      "Epoch 16. loss: 20.2774\n",
      "Epoch 17. loss: 20.2774\n",
      "Epoch 18. loss: 20.2774\n",
      "Epoch 19. loss: 20.2774\n",
      "Epoch 20. loss: 20.2774\n",
      "Epoch 21. loss: 20.2774\n"
     ]
    }
   ],
   "source": [
    "weights, bias, errors = optimize_perceptron(train_X, train_y, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_UJQikEuDFB",
    "outputId": "b704d1e7-4252-4474-9426-d659177ad54e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.1135\n"
     ]
    }
   ],
   "source": [
    "acc = calculate_accuracy(test_X, test_y, weights, bias)\n",
    "print('Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viZPqHHrsSgL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
